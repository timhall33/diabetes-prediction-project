{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8 & 9: Model Evaluation and Interpretation\n",
    "\n",
    "This notebook combines Phase 8 (Model Evaluation) and Phase 9 (Model Interpretation) into a comprehensive analysis:\n",
    "\n",
    "## Phase 8: Model Evaluation\n",
    "- Confusion matrices for all models\n",
    "- ROC curves and AUC comparison\n",
    "- Precision-Recall curves\n",
    "- Calibration curves (reliability diagrams)\n",
    "- Error analysis (what each model gets wrong)\n",
    "- Subgroup analysis (age, gender, BMI, race)\n",
    "- With-labs vs without-labs comparison\n",
    "- Regression evaluation (residuals, predicted vs actual)\n",
    "\n",
    "## Phase 9: Model Interpretation\n",
    "- SHAP values for best model (LightGBM)\n",
    "- Global feature importance\n",
    "- SHAP dependence plots\n",
    "- Modifiable vs non-modifiable risk factors\n",
    "- Actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ML imports\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.models.evaluate import (\n",
    "    compute_classification_metrics, compute_regression_metrics,\n",
    "    plot_confusion_matrix, plot_roc_curves, plot_precision_recall_curves,\n",
    "    plot_calibration_curve, plot_residuals, plot_predicted_vs_actual,\n",
    "    compare_models_table, DIABETES_LABELS, DIABETES_COLORS\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 100,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "})\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / 'data' / 'processed'\n",
    "INTERIM_DIR = project_root / 'data' / 'interim'\n",
    "MODELS_DIR = project_root / 'models' / 'advanced'\n",
    "FIGURES_DIR = project_root / 'reports' / 'figures'\n",
    "\n",
    "# Create figures directory\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature matrices\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "X_with_labs_minimal = pd.read_parquet(DATA_DIR / 'X_with_labs_minimal.parquet')\n",
    "X_with_labs_full = pd.read_parquet(DATA_DIR / 'X_with_labs_full.parquet')\n",
    "X_without_labs_minimal = pd.read_parquet(DATA_DIR / 'X_without_labs_minimal.parquet')\n",
    "X_without_labs_full = pd.read_parquet(DATA_DIR / 'X_without_labs_full.parquet')\n",
    "\n",
    "# Classification target\n",
    "y_classification = pd.read_parquet(DATA_DIR / 'y_with_labs_minimal.parquet')['DIABETES_STATUS']\n",
    "\n",
    "# Regression target (HbA1c)\n",
    "study_pop = pd.read_parquet(INTERIM_DIR / 'study_population.parquet')\n",
    "y_regression = study_pop.loc[y_classification.index, 'LBXGH']\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_with_labs_minimal:    {X_with_labs_minimal.shape}\")\n",
    "print(f\"  X_with_labs_full:       {X_with_labs_full.shape}\")\n",
    "print(f\"  X_without_labs_minimal: {X_without_labs_minimal.shape}\")\n",
    "print(f\"  X_without_labs_full:    {X_without_labs_full.shape}\")\n",
    "print(f\"  y_classification:       {y_classification.shape}\")\n",
    "print(f\"  y_regression:           {y_regression.shape} (non-null: {y_regression.notna().sum()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic data for subgroup analysis\n",
    "demographics = study_pop.loc[y_classification.index, ['RIDAGEYR', 'RIAGENDR', 'RIDRETH3']].copy()\n",
    "demographics['BMXBMI'] = study_pop.loc[y_classification.index, 'BMXBMI']\n",
    "\n",
    "# Create age groups\n",
    "demographics['AGE_GROUP'] = pd.cut(\n",
    "    demographics['RIDAGEYR'],\n",
    "    bins=[18, 40, 60, 100],\n",
    "    labels=['Young (18-39)', 'Middle (40-59)', 'Older (60+)']\n",
    ")\n",
    "\n",
    "# Create BMI categories\n",
    "demographics['BMI_CAT'] = pd.cut(\n",
    "    demographics['BMXBMI'],\n",
    "    bins=[0, 25, 30, 100],\n",
    "    labels=['Normal (<25)', 'Overweight (25-30)', 'Obese (30+)']\n",
    ")\n",
    "\n",
    "# Gender labels\n",
    "demographics['GENDER'] = demographics['RIAGENDR'].map({1: 'Male', 2: 'Female'})\n",
    "\n",
    "# Race/ethnicity labels (RIDRETH3)\n",
    "race_map = {\n",
    "    1: 'Mexican American',\n",
    "    2: 'Other Hispanic',\n",
    "    3: 'Non-Hispanic White',\n",
    "    4: 'Non-Hispanic Black',\n",
    "    6: 'Non-Hispanic Asian',\n",
    "    7: 'Other/Multi-Racial'\n",
    "}\n",
    "demographics['RACE'] = demographics['RIDRETH3'].map(race_map)\n",
    "\n",
    "print(\"Demographics loaded:\")\n",
    "print(demographics[['AGE_GROUP', 'GENDER', 'BMI_CAT', 'RACE']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# Classification models\n",
    "lgb_cls_with_labs = joblib.load(MODELS_DIR / 'classification' / 'lgb_with_labs.joblib')\n",
    "lgb_cls_without_labs = joblib.load(MODELS_DIR / 'classification' / 'lgb_without_labs.joblib')\n",
    "\n",
    "mlp_cls_with_labs_data = joblib.load(MODELS_DIR / 'classification' / 'mlp_with_labs.joblib')\n",
    "mlp_cls_without_labs_data = joblib.load(MODELS_DIR / 'classification' / 'mlp_without_labs.joblib')\n",
    "\n",
    "# MLP models come with scalers\n",
    "if isinstance(mlp_cls_with_labs_data, tuple):\n",
    "    mlp_cls_with_labs, scaler_cls_with = mlp_cls_with_labs_data\n",
    "else:\n",
    "    mlp_cls_with_labs = mlp_cls_with_labs_data\n",
    "    scaler_cls_with = None\n",
    "\n",
    "if isinstance(mlp_cls_without_labs_data, tuple):\n",
    "    mlp_cls_without_labs, scaler_cls_without = mlp_cls_without_labs_data\n",
    "else:\n",
    "    mlp_cls_without_labs = mlp_cls_without_labs_data\n",
    "    scaler_cls_without = None\n",
    "\n",
    "# Regression models\n",
    "lgb_reg_with_labs = joblib.load(MODELS_DIR / 'regression' / 'lgb_with_labs.joblib')\n",
    "lgb_reg_without_labs = joblib.load(MODELS_DIR / 'regression' / 'lgb_without_labs.joblib')\n",
    "\n",
    "mlp_reg_with_labs_data = joblib.load(MODELS_DIR / 'regression' / 'mlp_with_labs.joblib')\n",
    "mlp_reg_without_labs_data = joblib.load(MODELS_DIR / 'regression' / 'mlp_without_labs.joblib')\n",
    "\n",
    "if isinstance(mlp_reg_with_labs_data, tuple):\n",
    "    mlp_reg_with_labs, scaler_reg_with = mlp_reg_with_labs_data\n",
    "else:\n",
    "    mlp_reg_with_labs = mlp_reg_with_labs_data\n",
    "    scaler_reg_with = None\n",
    "\n",
    "if isinstance(mlp_reg_without_labs_data, tuple):\n",
    "    mlp_reg_without_labs, scaler_reg_without = mlp_reg_without_labs_data\n",
    "else:\n",
    "    mlp_reg_without_labs = mlp_reg_without_labs_data\n",
    "    scaler_reg_without = None\n",
    "\n",
    "print(\"\\nModels loaded:\")\n",
    "print(\"  Classification: LightGBM (with/without labs), MLP (with/without labs)\")\n",
    "print(\"  Regression: LightGBM (with/without labs), MLP (with/without labs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to load PyTorch model if available\npytorch_model = None\npytorch_scaler = None\npytorch_path = MODELS_DIR / 'classification' / 'pytorch_with_labs.pt'\n\nif pytorch_path.exists():\n    try:\n        import torch\n        import torch.nn as nn\n        \n        # Load the checkpoint\n        checkpoint = torch.load(pytorch_path, weights_only=False)\n        \n        # Define the model architecture (must match training)\n        class DiabetesClassifier(nn.Module):\n            def __init__(self, n_features, n_classes=3, dropout_rate=0.3):\n                super().__init__()\n                self.layer1 = nn.Sequential(\n                    nn.Linear(n_features, 128),\n                    nn.BatchNorm1d(128),\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate)\n                )\n                self.layer2 = nn.Sequential(\n                    nn.Linear(128, 64),\n                    nn.BatchNorm1d(64),\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate)\n                )\n                self.layer3 = nn.Sequential(\n                    nn.Linear(64, 32),\n                    nn.BatchNorm1d(32),\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate)\n                )\n                self.output = nn.Linear(32, n_classes)\n            \n            def forward(self, x):\n                x = self.layer1(x)\n                x = self.layer2(x)\n                x = self.layer3(x)\n                return self.output(x)\n        \n        # Get model parameters from checkpoint\n        n_features = checkpoint['n_features']\n        n_classes = checkpoint['n_classes']\n        dropout_rate = checkpoint['dropout_rate']\n        \n        # Create model and load state dict\n        pytorch_model = DiabetesClassifier(n_features, n_classes, dropout_rate)\n        pytorch_model.load_state_dict(checkpoint['model_state_dict'])\n        pytorch_model.eval()\n        \n        # Also get the scaler\n        pytorch_scaler = checkpoint.get('scaler')\n        \n        print(f\"PyTorch model loaded from {pytorch_path}\")\n        print(f\"  n_features: {n_features}, n_classes: {n_classes}\")\n        print(f\"  Saved metrics: {checkpoint.get('metrics', {})}\")\n    except Exception as e:\n        print(f\"Could not load PyTorch model: {e}\")\n        pytorch_model = None\nelse:\n    print(\"PyTorch model not found - will evaluate sklearn models only\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits (same as training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train+val (85%) and test (15%)\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    y_classification.index,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_classification\n",
    ")\n",
    "\n",
    "# Second split: train (70%) and val (15%) from train+val\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.15/0.85,  # 15% of original = 15/85 of train+val\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_classification.loc[train_val_idx]\n",
    ")\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"  Train: {len(train_idx):,} ({len(train_idx)/len(y_classification)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx):,} ({len(val_idx)/len(y_classification)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx):,} ({len(test_idx)/len(y_classification)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sets for each model type\n",
    "def get_test_data(X, y, test_idx):\n",
    "    \"\"\"Get test data with matching indices.\"\"\"\n",
    "    common_idx = test_idx.intersection(y.index)\n",
    "    return X.loc[common_idx], y.loc[common_idx]\n",
    "\n",
    "# Classification test sets\n",
    "X_test_lgb_with, y_test_cls = get_test_data(X_with_labs_minimal, y_classification, test_idx)\n",
    "X_test_lgb_without, _ = get_test_data(X_without_labs_minimal, y_classification, test_idx)\n",
    "X_test_mlp_with, _ = get_test_data(X_with_labs_full, y_classification, test_idx)\n",
    "X_test_mlp_without, _ = get_test_data(X_without_labs_full, y_classification, test_idx)\n",
    "\n",
    "# Regression test sets\n",
    "y_reg_valid = y_regression.dropna()\n",
    "X_test_lgb_with_reg, y_test_reg = get_test_data(X_with_labs_minimal, y_reg_valid, test_idx)\n",
    "X_test_lgb_without_reg, _ = get_test_data(X_without_labs_minimal, y_reg_valid, test_idx)\n",
    "X_test_mlp_with_reg, _ = get_test_data(X_with_labs_full, y_reg_valid, test_idx)\n",
    "X_test_mlp_without_reg, _ = get_test_data(X_without_labs_full, y_reg_valid, test_idx)\n",
    "\n",
    "print(f\"Classification test set: {len(y_test_cls)} samples\")\n",
    "print(f\"Regression test set: {len(y_test_reg)} samples (with valid HbA1c)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Classification Evaluation (Phase 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions for all classification models\npredictions = {}\nprobabilities = {}\n\n# LightGBM with labs\npredictions['LightGBM (with labs)'] = lgb_cls_with_labs.predict(X_test_lgb_with.values)\nprobabilities['LightGBM (with labs)'] = lgb_cls_with_labs.predict_proba(X_test_lgb_with.values)\n\n# LightGBM without labs\npredictions['LightGBM (without labs)'] = lgb_cls_without_labs.predict(X_test_lgb_without.values)\nprobabilities['LightGBM (without labs)'] = lgb_cls_without_labs.predict_proba(X_test_lgb_without.values)\n\n# MLP with labs\nif scaler_cls_with is not None:\n    X_test_scaled = scaler_cls_with.transform(X_test_mlp_with)\nelse:\n    X_test_scaled = X_test_mlp_with.values\npredictions['MLP (with labs)'] = mlp_cls_with_labs.predict(X_test_scaled)\nprobabilities['MLP (with labs)'] = mlp_cls_with_labs.predict_proba(X_test_scaled)\n\n# MLP without labs\nif scaler_cls_without is not None:\n    X_test_scaled = scaler_cls_without.transform(X_test_mlp_without)\nelse:\n    X_test_scaled = X_test_mlp_without.values\npredictions['MLP (without labs)'] = mlp_cls_without_labs.predict(X_test_scaled)\nprobabilities['MLP (without labs)'] = mlp_cls_without_labs.predict_proba(X_test_scaled)\n\n# PyTorch if available\nif pytorch_model is not None:\n    import torch\n    import torch.nn.functional as F\n    \n    # Use the saved scaler from the checkpoint\n    if pytorch_scaler is not None:\n        X_test_pytorch_scaled = pytorch_scaler.transform(X_test_mlp_with)\n    else:\n        X_test_pytorch_scaled = X_test_mlp_with.values\n    \n    with torch.no_grad():\n        X_tensor = torch.FloatTensor(X_test_pytorch_scaled)\n        logits = pytorch_model(X_tensor)\n        probs = F.softmax(logits, dim=1).numpy()\n        preds = np.argmax(probs, axis=1)\n    \n    predictions['PyTorch (with labs)'] = preds\n    probabilities['PyTorch (with labs)'] = probs\n\nprint(f\"Generated predictions for {len(predictions)} models\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all models\n",
    "results = {}\n",
    "for model_name in predictions:\n",
    "    y_pred = predictions[model_name]\n",
    "    y_prob = probabilities[model_name]\n",
    "    results[model_name] = compute_classification_metrics(y_test_cls.values, y_pred, y_prob)\n",
    "\n",
    "# Display comparison table\n",
    "metrics_df = compare_models_table(\n",
    "    results,\n",
    "    metrics=['accuracy', 'f1_macro', 'roc_auc_ovr', 'precision_macro', 'recall_macro'],\n",
    "    sort_by='f1_macro',\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION MODEL COMPARISON (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "n_models = len(predictions)\n",
    "n_cols = 2\n",
    "n_rows = (n_models + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "    plot_confusion_matrix(\n",
    "        y_test_cls.values, y_pred,\n",
    "        ax=axes[i],\n",
    "        normalize=True,\n",
    "        title=f'{model_name}\\nF1={results[model_name][\"f1_macro\"]:.3f}'\n",
    "    )\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices (Normalized) - Test Set', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models (one plot per class)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "class_names = {0: 'No Diabetes', 1: 'Prediabetes', 2: 'Diabetes'}\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(probabilities)))\n",
    "\n",
    "for class_idx, class_name in class_names.items():\n",
    "    ax = axes[class_idx]\n",
    "    \n",
    "    for (model_name, y_prob), color in zip(probabilities.items(), colors):\n",
    "        y_true_binary = (y_test_cls.values == class_idx).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        fpr, tpr, _ = roc_curve(y_true_binary, y_prob[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Shorten model name for legend\n",
    "        short_name = model_name.replace(' (with labs)', '+labs').replace(' (without labs)', '-labs')\n",
    "        ax.plot(fpr, tpr, color=color, linewidth=2, label=f'{short_name} ({roc_auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC Curve: {class_name}')\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ROC Curves by Class (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for class_idx, class_name in class_names.items():\n",
    "    ax = axes[class_idx]\n",
    "    \n",
    "    # Calculate prevalence for this class\n",
    "    prevalence = (y_test_cls.values == class_idx).mean()\n",
    "    \n",
    "    for (model_name, y_prob), color in zip(probabilities.items(), colors):\n",
    "        y_true_binary = (y_test_cls.values == class_idx).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "        precision, recall, _ = precision_recall_curve(y_true_binary, y_prob[:, class_idx])\n",
    "        ap = average_precision_score(y_true_binary, y_prob[:, class_idx])\n",
    "        \n",
    "        short_name = model_name.replace(' (with labs)', '+labs').replace(' (without labs)', '-labs')\n",
    "        ax.plot(recall, precision, color=color, linewidth=2, label=f'{short_name} (AP={ap:.3f})')\n",
    "    \n",
    "    # Baseline (prevalence)\n",
    "    ax.axhline(y=prevalence, color='gray', linestyle='--', alpha=0.5, label=f'Baseline ({prevalence:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(f'PR Curve: {class_name}')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('Precision-Recall Curves by Class', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_pr_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves for best model (LightGBM with labs) for all classes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "best_model = 'LightGBM (with labs)'\n",
    "y_prob = probabilities[best_model]\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "for class_idx, class_name in class_names.items():\n",
    "    ax = axes[class_idx]\n",
    "    \n",
    "    y_true_binary = (y_test_cls.values == class_idx).astype(int)\n",
    "    prob_class = y_prob[:, class_idx]\n",
    "    \n",
    "    # Calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true_binary, prob_class, n_bins=10)\n",
    "    \n",
    "    ax.plot(prob_pred, prob_true, 's-', color=DIABETES_COLORS[class_idx], \n",
    "            linewidth=2, markersize=8, label=f'{best_model}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfectly Calibrated')\n",
    "    \n",
    "    # Add histogram of predictions\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.hist(prob_class, bins=10, alpha=0.3, color=DIABETES_COLORS[class_idx])\n",
    "    ax2.set_ylabel('Count', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(f'Calibration: {class_name}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle(f'Calibration Curves - {best_model}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_calibration_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors for best model\n",
    "best_model = 'LightGBM (with labs)'\n",
    "y_pred = predictions[best_model]\n",
    "y_prob = probabilities[best_model]\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = pd.DataFrame({\n",
    "    'y_true': y_test_cls.values,\n",
    "    'y_pred': y_pred,\n",
    "    'correct': y_test_cls.values == y_pred,\n",
    "    'prob_0': y_prob[:, 0],\n",
    "    'prob_1': y_prob[:, 1],\n",
    "    'prob_2': y_prob[:, 2],\n",
    "    'max_prob': y_prob.max(axis=1),\n",
    "}, index=y_test_cls.index)\n",
    "\n",
    "# Add demographics\n",
    "error_df = error_df.join(demographics.loc[error_df.index])\n",
    "\n",
    "# Error rate by true class\n",
    "print(\"\\nError Rate by True Class:\")\n",
    "print(\"=\"*40)\n",
    "for cls in [0, 1, 2]:\n",
    "    mask = error_df['y_true'] == cls\n",
    "    error_rate = 1 - error_df.loc[mask, 'correct'].mean()\n",
    "    n = mask.sum()\n",
    "    print(f\"  {DIABETES_LABELS[cls]}: {error_rate:.1%} ({int(n * error_rate)}/{n} errors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What gets misclassified most often?\n",
    "print(\"\\nMisclassification Patterns:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "errors = error_df[~error_df['correct']].copy()\n",
    "\n",
    "# Cross-tabulate true vs predicted for errors\n",
    "error_patterns = pd.crosstab(\n",
    "    errors['y_true'].map(DIABETES_LABELS),\n",
    "    errors['y_pred'].map(DIABETES_LABELS),\n",
    "    margins=True\n",
    ")\n",
    "print(error_patterns)\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} ({len(errors)/len(error_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence in errors vs correct predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram of max probability for correct vs incorrect\n",
    "ax = axes[0]\n",
    "ax.hist(error_df.loc[error_df['correct'], 'max_prob'], bins=20, alpha=0.7, \n",
    "        label=f'Correct (n={error_df[\"correct\"].sum()})', color='green')\n",
    "ax.hist(error_df.loc[~error_df['correct'], 'max_prob'], bins=20, alpha=0.7,\n",
    "        label=f'Incorrect (n={(~error_df[\"correct\"]).sum()})', color='red')\n",
    "ax.set_xlabel('Max Predicted Probability (Confidence)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Prediction Confidence: Correct vs Incorrect')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Error rate by confidence bin\n",
    "ax = axes[1]\n",
    "error_df['confidence_bin'] = pd.cut(error_df['max_prob'], bins=[0, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0])\n",
    "error_by_conf = error_df.groupby('confidence_bin', observed=True)['correct'].agg(['mean', 'count'])\n",
    "error_by_conf['error_rate'] = 1 - error_by_conf['mean']\n",
    "\n",
    "x = range(len(error_by_conf))\n",
    "ax.bar(x, error_by_conf['error_rate'], color='coral', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(i) for i in error_by_conf.index], rotation=45, ha='right')\n",
    "ax.set_xlabel('Confidence Bin')\n",
    "ax.set_ylabel('Error Rate')\n",
    "ax.set_title('Error Rate by Prediction Confidence')\n",
    "\n",
    "# Add count labels\n",
    "for i, (idx, row) in enumerate(error_by_conf.iterrows()):\n",
    "    ax.annotate(f'n={int(row[\"count\"])}', xy=(i, row['error_rate']), \n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Error Analysis - {best_model}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subgroup(df, subgroup_col, model_name='LightGBM (with labs)'):\n",
    "    \"\"\"Evaluate model performance by subgroup.\"\"\"\n",
    "    y_pred = predictions[model_name]\n",
    "    y_prob = probabilities[model_name]\n",
    "    \n",
    "    results = []\n",
    "    for group in df[subgroup_col].dropna().unique():\n",
    "        mask = df[subgroup_col] == group\n",
    "        if mask.sum() < 50:  # Skip small groups\n",
    "            continue\n",
    "        \n",
    "        y_true_group = y_test_cls.values[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        y_prob_group = y_prob[mask]\n",
    "        \n",
    "        metrics = compute_classification_metrics(y_true_group, y_pred_group, y_prob_group)\n",
    "        results.append({\n",
    "            'Subgroup': group,\n",
    "            'N': mask.sum(),\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'F1 Macro': metrics['f1_macro'],\n",
    "            'ROC AUC': metrics.get('roc_auc_ovr', np.nan)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('Subgroup')\n",
    "\n",
    "# Match error_df index\n",
    "subgroup_df = error_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Age Group\n",
    "print(\"\\nPerformance by Age Group:\")\n",
    "print(\"=\"*60)\n",
    "age_results = evaluate_subgroup(subgroup_df, 'AGE_GROUP')\n",
    "print(age_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Gender\n",
    "print(\"\\nPerformance by Gender:\")\n",
    "print(\"=\"*60)\n",
    "gender_results = evaluate_subgroup(subgroup_df, 'GENDER')\n",
    "print(gender_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by BMI Category\n",
    "print(\"\\nPerformance by BMI Category:\")\n",
    "print(\"=\"*60)\n",
    "bmi_results = evaluate_subgroup(subgroup_df, 'BMI_CAT')\n",
    "print(bmi_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Race/Ethnicity\n",
    "print(\"\\nPerformance by Race/Ethnicity:\")\n",
    "print(\"=\"*60)\n",
    "race_results = evaluate_subgroup(subgroup_df, 'RACE')\n",
    "print(race_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize subgroup performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Age\n",
    "ax = axes[0, 0]\n",
    "if len(age_results) > 0:\n",
    "    x = range(len(age_results))\n",
    "    ax.bar(x, age_results['F1 Macro'], color='steelblue', alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(age_results['Subgroup'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('F1 Macro')\n",
    "    ax.set_title('Performance by Age Group')\n",
    "    ax.set_ylim([0, 0.8])\n",
    "    for i, (_, row) in enumerate(age_results.iterrows()):\n",
    "        ax.annotate(f'{row[\"F1 Macro\"]:.3f}\\n(n={row[\"N\"]})', xy=(i, row['F1 Macro']+0.02), ha='center', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Gender\n",
    "ax = axes[0, 1]\n",
    "if len(gender_results) > 0:\n",
    "    x = range(len(gender_results))\n",
    "    ax.bar(x, gender_results['F1 Macro'], color='coral', alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(gender_results['Subgroup'])\n",
    "    ax.set_ylabel('F1 Macro')\n",
    "    ax.set_title('Performance by Gender')\n",
    "    ax.set_ylim([0, 0.8])\n",
    "    for i, (_, row) in enumerate(gender_results.iterrows()):\n",
    "        ax.annotate(f'{row[\"F1 Macro\"]:.3f}\\n(n={row[\"N\"]})', xy=(i, row['F1 Macro']+0.02), ha='center', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# BMI\n",
    "ax = axes[1, 0]\n",
    "if len(bmi_results) > 0:\n",
    "    x = range(len(bmi_results))\n",
    "    ax.bar(x, bmi_results['F1 Macro'], color='green', alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(bmi_results['Subgroup'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('F1 Macro')\n",
    "    ax.set_title('Performance by BMI Category')\n",
    "    ax.set_ylim([0, 0.8])\n",
    "    for i, (_, row) in enumerate(bmi_results.iterrows()):\n",
    "        ax.annotate(f'{row[\"F1 Macro\"]:.3f}\\n(n={row[\"N\"]})', xy=(i, row['F1 Macro']+0.02), ha='center', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Race\n",
    "ax = axes[1, 1]\n",
    "if len(race_results) > 0:\n",
    "    x = range(len(race_results))\n",
    "    ax.bar(x, race_results['F1 Macro'], color='purple', alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(race_results['Subgroup'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('F1 Macro')\n",
    "    ax.set_title('Performance by Race/Ethnicity')\n",
    "    ax.set_ylim([0, 0.8])\n",
    "    for i, (_, row) in enumerate(race_results.iterrows()):\n",
    "        ax.annotate(f'{row[\"F1 Macro\"]:.3f}', xy=(i, row['F1 Macro']+0.02), ha='center', fontsize=8, rotation=90)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Model Performance by Subgroup - LightGBM (with labs)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_subgroup_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. With-Labs vs Without-Labs Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with-labs vs without-labs performance\n",
    "comparison_data = []\n",
    "\n",
    "for model_type in ['LightGBM', 'MLP']:\n",
    "    with_labs_key = f'{model_type} (with labs)'\n",
    "    without_labs_key = f'{model_type} (without labs)'\n",
    "    \n",
    "    if with_labs_key in results and without_labs_key in results:\n",
    "        with_labs_f1 = results[with_labs_key]['f1_macro']\n",
    "        without_labs_f1 = results[without_labs_key]['f1_macro']\n",
    "        with_labs_auc = results[with_labs_key]['roc_auc_ovr']\n",
    "        without_labs_auc = results[without_labs_key]['roc_auc_ovr']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_type,\n",
    "            'F1 (with labs)': with_labs_f1,\n",
    "            'F1 (without labs)': without_labs_f1,\n",
    "            'F1 Drop': with_labs_f1 - without_labs_f1,\n",
    "            'F1 Drop %': (with_labs_f1 - without_labs_f1) / with_labs_f1 * 100,\n",
    "            'AUC (with labs)': with_labs_auc,\n",
    "            'AUC (without labs)': without_labs_auc,\n",
    "            'AUC Drop': with_labs_auc - without_labs_auc,\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nWith-Labs vs Without-Labs Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# F1 comparison\n",
    "ax = axes[0]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison_df['F1 (with labs)'], width, label='With Labs', color='#2166ac', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison_df['F1 (without labs)'], width, label='Without Labs', color='#b2182b', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.set_ylabel('F1 Macro')\n",
    "ax.set_title('F1 Score: With vs Without Labs')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add drop annotations\n",
    "for i, row in comparison_df.iterrows():\n",
    "    ax.annotate(f'-{row[\"F1 Drop %\"]:.1f}%', \n",
    "                xy=(i + width/2, row['F1 (without labs)'] + 0.02),\n",
    "                ha='center', fontsize=10, color='red')\n",
    "\n",
    "# AUC comparison\n",
    "ax = axes[1]\n",
    "ax.bar(x - width/2, comparison_df['AUC (with labs)'], width, label='With Labs', color='#2166ac', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison_df['AUC (without labs)'], width, label='Without Labs', color='#b2182b', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.set_ylabel('ROC AUC')\n",
    "ax.set_title('ROC AUC: With vs Without Labs')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Impact of Laboratory Features on Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_labs_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Regression Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression predictions\n",
    "reg_predictions = {}\n",
    "\n",
    "# LightGBM with labs\n",
    "reg_predictions['LightGBM (with labs)'] = lgb_reg_with_labs.predict(X_test_lgb_with_reg.values)\n",
    "\n",
    "# LightGBM without labs\n",
    "reg_predictions['LightGBM (without labs)'] = lgb_reg_without_labs.predict(X_test_lgb_without_reg.values)\n",
    "\n",
    "# MLP with labs\n",
    "if scaler_reg_with is not None:\n",
    "    X_scaled = scaler_reg_with.transform(X_test_mlp_with_reg)\n",
    "else:\n",
    "    X_scaled = X_test_mlp_with_reg.values\n",
    "reg_predictions['MLP (with labs)'] = mlp_reg_with_labs.predict(X_scaled)\n",
    "\n",
    "# MLP without labs\n",
    "if scaler_reg_without is not None:\n",
    "    X_scaled = scaler_reg_without.transform(X_test_mlp_without_reg)\n",
    "else:\n",
    "    X_scaled = X_test_mlp_without_reg.values\n",
    "reg_predictions['MLP (without labs)'] = mlp_reg_without_labs.predict(X_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "reg_results = {}\n",
    "for model_name, y_pred in reg_predictions.items():\n",
    "    reg_results[model_name] = compute_regression_metrics(y_test_reg.values, y_pred)\n",
    "\n",
    "# Display comparison\n",
    "reg_df = compare_models_table(\n",
    "    reg_results,\n",
    "    metrics=['rmse', 'mae', 'r2', 'mape'],\n",
    "    sort_by='rmse',\n",
    "    ascending=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REGRESSION MODEL COMPARISON (Test Set) - HbA1c Prediction\")\n",
    "print(\"=\"*70)\n",
    "print(reg_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for best regression model\n",
    "best_reg_model = 'LightGBM (with labs)'\n",
    "y_pred_reg = reg_predictions[best_reg_model]\n",
    "\n",
    "fig = plot_residuals(y_test_reg.values, y_pred_reg, title=f'{best_reg_model} - Residual Analysis')\n",
    "fig.savefig(FIGURES_DIR / 'phase8_regression_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual for all regression models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(reg_predictions.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.scatter(y_pred, y_test_reg.values, alpha=0.3, s=10)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test_reg.values.min(), y_pred.min())\n",
    "    max_val = max(y_test_reg.values.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    \n",
    "    # Add metrics\n",
    "    r2 = reg_results[model_name]['r2']\n",
    "    rmse = reg_results[model_name]['rmse']\n",
    "    ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}\\nRMSE = {rmse:.3f}',\n",
    "            transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('Predicted HbA1c (%)')\n",
    "    ax.set_ylabel('Actual HbA1c (%)')\n",
    "    ax.set_title(model_name)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Predicted vs Actual HbA1c - All Models', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase8_regression_predicted_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Model Interpretation (Phase 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. SHAP Analysis (LightGBM)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values explain how each feature contributes to the prediction:\n",
    "- **Positive SHAP value**: Feature pushes prediction toward higher class\n",
    "- **Negative SHAP value**: Feature pushes prediction toward lower class\n",
    "- **Magnitude**: Importance of the feature for that prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create SHAP explainer for LightGBM\nprint(\"Creating SHAP explainer for LightGBM (with labs)...\")\nprint(\"This uses TreeExplainer which is fast for tree-based models.\")\n\nexplainer = shap.TreeExplainer(lgb_cls_with_labs)\n\n# Calculate SHAP values for test set\n# Using a sample for faster computation (full test set would also work)\nn_samples = min(1000, len(X_test_lgb_with))\nX_shap = X_test_lgb_with.sample(n=n_samples, random_state=RANDOM_STATE)\n\n# Store feature names before computing SHAP values\nfeature_names = X_shap.columns.tolist()\n\nprint(f\"\\nComputing SHAP values for {n_samples} samples...\")\n# Compute SHAP values - pass numpy array\nshap_values_raw = explainer.shap_values(X_shap.values)\n\n# Handle different SHAP output formats:\n# - Old format (list): shap_values[class_idx] gives shape (n_samples, n_features)\n# - New format (3D array): shape (n_samples, n_features, n_classes)\nif isinstance(shap_values_raw, list):\n    # Old format - list of arrays\n    shap_values = shap_values_raw\n    print(f\"SHAP values format: list of {len(shap_values)} arrays, each {shap_values[0].shape}\")\nelse:\n    # New format (SHAP 0.50+) - 3D array (n_samples, n_features, n_classes)\n    # Convert to list format for compatibility with our plotting code\n    n_classes = shap_values_raw.shape[2]\n    shap_values = [shap_values_raw[:, :, i] for i in range(n_classes)]\n    print(f\"SHAP values format: 3D array {shap_values_raw.shape} -> converted to list\")\n\nprint(f\"Features shape: {X_shap.shape}\")\nprint(f\"Number of classes: {len(shap_values)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SHAP Summary plot (beeswarm) for each class\nfig, axes = plt.subplots(1, 3, figsize=(18, 8))\n\nfor class_idx, class_name in DIABETES_LABELS.items():\n    plt.sca(axes[class_idx])\n    shap.summary_plot(\n        shap_values[class_idx], \n        X_shap.values,  # Use numpy array\n        feature_names=feature_names,  # Explicitly pass feature names\n        max_display=15,\n        show=False,\n        plot_size=None\n    )\n    axes[class_idx].set_title(f'SHAP Summary: {class_name}', fontsize=12)\n\nplt.suptitle('SHAP Feature Importance by Class', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / 'phase9_shap_summary_by_class.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global feature importance (mean absolute SHAP across all classes)\n# Average SHAP values across classes\nmean_shap = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n\n# Create importance dataframe\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Mean |SHAP|': mean_shap\n}).sort_values('Mean |SHAP|', ascending=False)\n\n# Display top 20\nprint(\"\\nTop 20 Most Important Features (Mean |SHAP|):\")\nprint(\"=\"*50)\nprint(importance_df.head(20).to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of global feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "y_pos = np.arange(top_n)\n",
    "ax.barh(y_pos, top_features['Mean |SHAP|'], color='steelblue', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.invert_yaxis()  # Top feature at top\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_title('Top 20 Features by SHAP Importance', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase9_shap_importance_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SHAP dependence plots for top features (for Diabetes class)\ndiabetes_class_idx = 2  # Diabetes\ntop_features_list = importance_df.head(6)['Feature'].tolist()\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, feature in enumerate(top_features_list):\n    ax = axes[i]\n    # Get feature index\n    feature_idx = feature_names.index(feature)\n    shap.dependence_plot(\n        feature_idx,  # Use index instead of name\n        shap_values[diabetes_class_idx],\n        X_shap.values,\n        feature_names=feature_names,\n        ax=ax,\n        show=False\n    )\n    ax.set_title(f'{feature}', fontsize=11)\n\nplt.suptitle('SHAP Dependence Plots - Top Features (Diabetes Class)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / 'phase9_shap_dependence.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Permutation importance (model-agnostic)\nprint(\"Computing permutation importance (this may take a minute)...\")\n\n# Use a subset for speed\nn_perm = min(500, len(X_test_lgb_with))\nX_perm = X_test_lgb_with.sample(n=n_perm, random_state=RANDOM_STATE)\ny_perm = y_test_cls.loc[X_perm.index]\n\nperm_importance = permutation_importance(\n    lgb_cls_with_labs,\n    X_perm.values,  # Use numpy array\n    y_perm.values,  # Use numpy array\n    n_repeats=10,\n    random_state=RANDOM_STATE,\n    scoring='f1_macro',\n    n_jobs=-1\n)\n\nperm_df = pd.DataFrame({\n    'Feature': X_perm.columns,\n    'Importance': perm_importance.importances_mean,\n    'Std': perm_importance.importances_std\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\nTop 20 Features by Permutation Importance:\")\nprint(\"=\"*50)\nprint(perm_df.head(20).to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SHAP vs Permutation importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "# SHAP\n",
    "ax = axes[0]\n",
    "top_shap = importance_df.head(top_n)\n",
    "y_pos = np.arange(top_n)\n",
    "ax.barh(y_pos, top_shap['Mean |SHAP|'], color='steelblue', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_shap['Feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_title('SHAP Importance', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Permutation\n",
    "ax = axes[1]\n",
    "top_perm = perm_df.head(top_n)\n",
    "ax.barh(y_pos, top_perm['Importance'], xerr=top_perm['Std'], color='coral', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_perm['Feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean F1 Decrease')\n",
    "ax.set_title('Permutation Importance', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Feature Importance: SHAP vs Permutation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase9_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Modifiable vs Non-Modifiable Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features as modifiable vs non-modifiable\n",
    "modifiable_patterns = [\n",
    "    'BMX',  # Body measures (weight, BMI, waist)\n",
    "    'DR1',  # Dietary intake\n",
    "    'PAQ', 'PAD',  # Physical activity\n",
    "    'SL',  # Sleep\n",
    "    'ALQ',  # Alcohol\n",
    "    'SMQ', 'SMD',  # Smoking\n",
    "    'BPX',  # Blood pressure (partially modifiable)\n",
    "    'WEIGHT',  # Derived weight features\n",
    "    'WAIST',  # Waist-height ratio\n",
    "    'PHQ9',  # Depression (treatable)\n",
    "    'TOTAL_WATER',\n",
    "    'CARB', 'SUGAR', 'SAT_FAT',  # Dietary derived\n",
    "    'AVG_SYS', 'AVG_DIA', 'PULSE', 'MAP', 'BP_',  # BP derived\n",
    "]\n",
    "\n",
    "non_modifiable_patterns = [\n",
    "    'RIDAGEYR',  # Age\n",
    "    'RIAGENDR',  # Gender\n",
    "    'MCQ300C',  # Family history\n",
    "    'MCQ160',  # Medical history (CHF, CHD, etc.)\n",
    "    'ANY_CVD',  # CVD history\n",
    "]\n",
    "\n",
    "def categorize_feature(feature):\n",
    "    for pattern in non_modifiable_patterns:\n",
    "        if pattern in feature:\n",
    "            return 'Non-Modifiable'\n",
    "    for pattern in modifiable_patterns:\n",
    "        if pattern in feature:\n",
    "            return 'Modifiable'\n",
    "    # Lab values are not directly modifiable (but reflect modifiable factors)\n",
    "    if feature.startswith('LB') or feature.startswith('UR') or 'ACR' in feature or 'TG_HDL' in feature:\n",
    "        return 'Lab Value'\n",
    "    return 'Other'\n",
    "\n",
    "importance_df['Category'] = importance_df['Feature'].apply(categorize_feature)\n",
    "\n",
    "# Summary by category\n",
    "category_summary = importance_df.groupby('Category')['Mean |SHAP|'].agg(['sum', 'mean', 'count'])\n",
    "category_summary.columns = ['Total Importance', 'Mean Importance', 'N Features']\n",
    "category_summary = category_summary.sort_values('Total Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance by Category:\")\n",
    "print(\"=\"*60)\n",
    "print(category_summary.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top modifiable features\n",
    "print(\"\\nTop 15 Modifiable Risk Factors:\")\n",
    "print(\"=\"*50)\n",
    "modifiable_df = importance_df[importance_df['Category'] == 'Modifiable'].head(15)\n",
    "print(modifiable_df[['Feature', 'Mean |SHAP|']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize modifiable vs non-modifiable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Pie chart of total importance by category\n",
    "ax = axes[0]\n",
    "colors = {'Modifiable': '#2ca02c', 'Non-Modifiable': '#d62728', 'Lab Value': '#1f77b4', 'Other': '#7f7f7f'}\n",
    "cat_colors = [colors.get(cat, '#7f7f7f') for cat in category_summary.index]\n",
    "ax.pie(category_summary['Total Importance'], labels=category_summary.index, autopct='%1.1f%%',\n",
    "       colors=cat_colors, startangle=90)\n",
    "ax.set_title('Share of Total SHAP Importance', fontsize=12)\n",
    "\n",
    "# Bar chart of top features colored by category\n",
    "ax = axes[1]\n",
    "top_20 = importance_df.head(20)\n",
    "y_pos = np.arange(20)\n",
    "bar_colors = [colors.get(cat, '#7f7f7f') for cat in top_20['Category']]\n",
    "ax.barh(y_pos, top_20['Mean |SHAP|'], color=bar_colors, alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_20['Feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_title('Top 20 Features (Colored by Category)', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors[k], label=k) for k in colors]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.suptitle('Modifiable vs Non-Modifiable Risk Factors', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase9_modifiable_factors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Actionable Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIONABLE INSIGHTS FOR DIABETES PREVENTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nð MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "best_model = 'LightGBM (with labs)'\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"  - F1 Macro: {results[best_model]['f1_macro']:.3f}\")\n",
    "print(f\"  - ROC AUC: {results[best_model]['roc_auc_ovr']:.3f}\")\n",
    "print(f\"  - Accuracy: {results[best_model]['accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\nð¯ TOP MODIFIABLE RISK FACTORS\")\n",
    "print(\"-\" * 40)\n",
    "for i, (_, row) in enumerate(modifiable_df.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2}. {row['Feature']}: {row['Mean |SHAP|']:.4f}\")\n",
    "\n",
    "print(\"\\nð¡ KEY RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "recommendations = [\n",
    "    \"1. WEIGHT MANAGEMENT: BMI and waist circumference are top predictors.\"\n",
    "    \"   Maintaining healthy weight is the most impactful modifiable factor.\",\n",
    "    \"2. PHYSICAL ACTIVITY: Regular exercise reduces diabetes risk.\"\n",
    "    \"   Both vigorous and moderate activity contribute.\",\n",
    "    \"3. DIETARY QUALITY: Monitor carbohydrate quality (fiber ratio),\"\n",
    "    \"   saturated fat intake, and sugar consumption.\",\n",
    "    \"4. BLOOD PRESSURE: Hypertension is strongly associated with diabetes.\"\n",
    "    \"   Regular monitoring and management is important.\",\n",
    "    \"5. SLEEP: Sleep duration and quality affect metabolic health.\"\n",
    "    \"   Aim for consistent 7-8 hours per night.\",\n",
    "]\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "    print()\n",
    "\n",
    "print(\"\\nâ ï¸ LIMITATIONS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"- Cross-sectional data: Cannot establish causation\")\n",
    "print(\"- Prediabetes is hardest to predict (F1 ~0.56)\")\n",
    "print(\"- Lab values provide ~10% F1 improvement\")\n",
    "print(\"- Model performs slightly worse for older age groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "evaluation_results = {\n",
    "    'classification': {\n",
    "        model: {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                for k, v in metrics.items() if k != 'confusion_matrix'}\n",
    "        for model, metrics in results.items()\n",
    "    },\n",
    "    'regression': {\n",
    "        model: {k: float(v) for k, v in metrics.items()}\n",
    "        for model, metrics in reg_results.items()\n",
    "    },\n",
    "    'feature_importance': {\n",
    "        'shap_top_20': importance_df.head(20).to_dict('records'),\n",
    "        'permutation_top_20': perm_df.head(20).to_dict('records'),\n",
    "        'importance_by_category': category_summary.to_dict(),\n",
    "    },\n",
    "    'subgroup_analysis': {\n",
    "        'by_age': age_results.to_dict('records') if len(age_results) > 0 else [],\n",
    "        'by_gender': gender_results.to_dict('records') if len(gender_results) > 0 else [],\n",
    "        'by_bmi': bmi_results.to_dict('records') if len(bmi_results) > 0 else [],\n",
    "        'by_race': race_results.to_dict('records') if len(race_results) > 0 else [],\n",
    "    },\n",
    "    'labs_comparison': comparison_df.to_dict('records'),\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_path = MODELS_DIR / 'evaluation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance to CSV for easy reference\n",
    "importance_df.to_csv(MODELS_DIR / 'feature_importance_shap.csv', index=False)\n",
    "perm_df.to_csv(MODELS_DIR / 'feature_importance_permutation.csv', index=False)\n",
    "\n",
    "print(f\"Feature importance saved to:\")\n",
    "print(f\"  - {MODELS_DIR / 'feature_importance_shap.csv'}\")\n",
    "print(f\"  - {MODELS_DIR / 'feature_importance_permutation.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 8 & 9 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nð FIGURES GENERATED:\")\n",
    "figures = list(FIGURES_DIR.glob('phase8_*.png')) + list(FIGURES_DIR.glob('phase9_*.png'))\n",
    "for fig_path in sorted(figures):\n",
    "    print(f\"  - {fig_path.name}\")\n",
    "\n",
    "print(f\"\\nð ARTIFACTS SAVED:\")\n",
    "print(f\"  - {MODELS_DIR / 'evaluation_results.json'}\")\n",
    "print(f\"  - {MODELS_DIR / 'feature_importance_shap.csv'}\")\n",
    "print(f\"  - {MODELS_DIR / 'feature_importance_permutation.csv'}\")\n",
    "\n",
    "print(\"\\nâ KEY FINDINGS:\")\n",
    "print(f\"  1. Best classification model: LightGBM (with labs) - F1={results['LightGBM (with labs)']['f1_macro']:.3f}\")\n",
    "print(f\"  2. Best regression model: LightGBM (with labs) - RÂ²={reg_results['LightGBM (with labs)']['r2']:.3f}\")\n",
    "print(f\"  3. Labs provide ~{comparison_df.iloc[0]['F1 Drop %']:.1f}% F1 improvement for LightGBM\")\n",
    "print(f\"  4. Top modifiable factors: BMI, waist, dietary patterns, physical activity\")\n",
    "print(f\"  5. Prediabetes is hardest to predict (lowest per-class F1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- **Phase 10**: Deployment (Streamlit app, API)\n",
    "- **Phase 11**: Documentation & Polish (README, final report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}