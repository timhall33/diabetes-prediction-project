{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7: Advanced Models\n",
    "\n",
    "This notebook trains advanced models with hyperparameter tuning:\n",
    "- **LightGBM**: Gradient boosting that handles NaN natively (uses `minimal` datasets)\n",
    "- **MLP (sklearn)**: Neural network requiring complete data (uses `full` datasets)\n",
    "\n",
    "## MLflow Tracking\n",
    "\n",
    "We use MLflow to track experiments:\n",
    "- **Parameters**: Hyperparameters for each model\n",
    "- **Metrics**: F1, accuracy, AUC (classification); RMSE, MAE, R² (regression)\n",
    "- **Artifacts**: Trained models, confusion matrices, ROC curves\n",
    "- **Tags**: Model type, feature set, task type\n",
    "\n",
    "To view the MLflow UI after running this notebook:\n",
    "```bash\n",
    "cd /path/to/project\n",
    "mlflow ui --backend-store-uri mlruns\n",
    "```\n",
    "Then open http://localhost:5000 in your browser.\n",
    "\n",
    "## Optuna Hyperparameter Optimization\n",
    "\n",
    "We use Optuna's TPE (Tree-structured Parzen Estimator) sampler for Bayesian optimization:\n",
    "- More efficient than grid/random search\n",
    "- Learns from previous trials to focus on promising regions\n",
    "- Handles conditional hyperparameters (e.g., layer sizes in MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ML imports\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import optuna\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.models.train import (\n",
    "    split_data, get_class_weights, setup_mlflow, log_experiment,\n",
    "    tune_model, save_model, train_with_cv\n",
    ")\n",
    "from src.models.evaluate import (\n",
    "    compute_classification_metrics, compute_regression_metrics,\n",
    "    plot_confusion_matrix, plot_roc_curves, plot_residuals,\n",
    "    compare_models_table, plot_model_comparison, plot_feature_set_comparison,\n",
    "    generate_classification_report_figures\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nRANDOM_STATE = 42\nCV_FOLDS = 3  # 3-fold CV for faster iteration\n\n# Trial counts - adjust these for quick test vs full run\n# Quick test: 10/5 trials (~15 min total)\n# Full run: 100/50 trials (~10 hours total)\nN_TRIALS_LIGHTGBM = 10  # Quick test (set to 100 for full run)\nN_TRIALS_MLP = 5  # Quick test (set to 50 for full run)\n\n# Paths\nDATA_DIR = project_root / 'data' / 'processed'\nMODELS_DIR = project_root / 'models'\nFIGURES_DIR = project_root / 'reports' / 'figures'\nMLRUNS_DIR = project_root / 'mlruns'\n\n# Create directories\nMODELS_DIR.mkdir(exist_ok=True)\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Models directory: {MODELS_DIR}\")\nprint(f\"Figures directory: {FIGURES_DIR}\")\nprint(f\"\\nTrial configuration:\")\nprint(f\"  LightGBM: {N_TRIALS_LIGHTGBM} trials\")\nprint(f\"  MLP: {N_TRIALS_MLP} trials\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We have 4 feature datasets:\n",
    "- `X_with_labs_minimal`: Has NaN, for LightGBM (109 features)\n",
    "- `X_with_labs_full`: No NaN, for MLP (96 features)\n",
    "- `X_without_labs_minimal`: Has NaN, for LightGBM (92 features)\n",
    "- `X_without_labs_full`: No NaN, for MLP (82 features)\n",
    "\n",
    "And 2 target variables:\n",
    "- `y_classification`: 3-class diabetes status (0, 1, 2)\n",
    "- `y_regression`: HbA1c level (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load all datasets\nprint(\"Loading datasets...\")\n\n# Feature matrices\nX_with_labs_minimal = pd.read_parquet(DATA_DIR / 'X_with_labs_minimal.parquet')\nX_with_labs_full = pd.read_parquet(DATA_DIR / 'X_with_labs_full.parquet')\nX_without_labs_minimal = pd.read_parquet(DATA_DIR / 'X_without_labs_minimal.parquet')\nX_without_labs_full = pd.read_parquet(DATA_DIR / 'X_without_labs_full.parquet')\n\n# Target variables\n# Classification target: DIABETES_STATUS (0, 1, 2) from any y file\ny_classification = pd.read_parquet(DATA_DIR / 'y_with_labs_minimal.parquet')['DIABETES_STATUS']\n\n# Regression target: LBXGH (HbA1c) from study_population\n# (not in processed y files because it's a target, not a feature)\nINTERIM_DIR = project_root / 'data' / 'interim'\nstudy_pop = pd.read_parquet(INTERIM_DIR / 'study_population.parquet')\ny_regression = study_pop.loc[y_classification.index, 'LBXGH']\n\nprint(\"\\nDataset shapes:\")\nprint(f\"  X_with_labs_minimal:    {X_with_labs_minimal.shape} (has NaN: {X_with_labs_minimal.isna().any().any()})\")\nprint(f\"  X_with_labs_full:       {X_with_labs_full.shape} (has NaN: {X_with_labs_full.isna().any().any()})\")\nprint(f\"  X_without_labs_minimal: {X_without_labs_minimal.shape} (has NaN: {X_without_labs_minimal.isna().any().any()})\")\nprint(f\"  X_without_labs_full:    {X_without_labs_full.shape} (has NaN: {X_without_labs_full.isna().any().any()})\")\nprint(f\"\\n  y_classification:       {y_classification.shape}\")\nprint(f\"  y_regression:           {y_regression.shape} (non-null: {y_regression.notna().sum()})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target distributions\n",
    "print(\"Target variable distributions:\")\n",
    "print(\"\\nClassification (DIABETES_STATUS):\")\n",
    "status_counts = y_classification.value_counts().sort_index()\n",
    "status_labels = {0: 'No Diabetes', 1: 'Prediabetes', 2: 'Diabetes'}\n",
    "for idx, count in status_counts.items():\n",
    "    print(f\"  {status_labels[idx]}: {count:,} ({count/len(y_classification)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRegression (HbA1c):\")\n",
    "print(f\"  Range: {y_regression.min():.1f}% - {y_regression.max():.1f}%\")\n",
    "print(f\"  Mean:  {y_regression.mean():.2f}%\")\n",
    "print(f\"  Std:   {y_regression.std():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting\n",
    "\n",
    "Split data into train (70%), validation (15%), and test (15%) sets.\n",
    "- Stratified by target class for classification\n",
    "- Same random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split classification data (using with_labs_minimal as reference for indices)\n",
    "X_train_idx, X_val_idx, X_test_idx, y_train_cls, y_val_cls, y_test_cls = split_data(\n",
    "    X_with_labs_minimal, y_classification,\n",
    "    test_size=0.15, val_size=0.15, random_state=RANDOM_STATE, stratify=True\n",
    ")\n",
    "\n",
    "# Get indices for splitting other datasets consistently\n",
    "train_idx = X_train_idx.index\n",
    "val_idx = X_val_idx.index\n",
    "test_idx = X_test_idx.index\n",
    "\n",
    "print(f\"Data split sizes:\")\n",
    "print(f\"  Train: {len(train_idx):,} ({len(train_idx)/len(y_classification)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx):,} ({len(val_idx)/len(y_classification)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx):,} ({len(test_idx)/len(y_classification)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all data splits using consistent indices\n",
    "def create_splits(X, y, train_idx, val_idx, test_idx):\n",
    "    \"\"\"Create train/val/test splits using pre-defined indices.\"\"\"\n",
    "    # Handle case where y might have different index (regression has fewer samples)\n",
    "    common_train = train_idx.intersection(y.index)\n",
    "    common_val = val_idx.intersection(y.index)\n",
    "    common_test = test_idx.intersection(y.index)\n",
    "    \n",
    "    X_train = X.loc[common_train]\n",
    "    X_val = X.loc[common_val]\n",
    "    X_test = X.loc[common_test]\n",
    "    y_train = y.loc[common_train]\n",
    "    y_val = y.loc[common_val]\n",
    "    y_test = y.loc[common_test]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Classification splits\n",
    "splits_cls = {\n",
    "    'with_labs_minimal': create_splits(X_with_labs_minimal, y_classification, train_idx, val_idx, test_idx),\n",
    "    'with_labs_full': create_splits(X_with_labs_full, y_classification, train_idx, val_idx, test_idx),\n",
    "    'without_labs_minimal': create_splits(X_without_labs_minimal, y_classification, train_idx, val_idx, test_idx),\n",
    "    'without_labs_full': create_splits(X_without_labs_full, y_classification, train_idx, val_idx, test_idx),\n",
    "}\n",
    "\n",
    "# Regression splits (fewer samples - only those with HbA1c values)\n",
    "splits_reg = {\n",
    "    'with_labs_minimal': create_splits(X_with_labs_minimal, y_regression.dropna(), train_idx, val_idx, test_idx),\n",
    "    'with_labs_full': create_splits(X_with_labs_full, y_regression.dropna(), train_idx, val_idx, test_idx),\n",
    "    'without_labs_minimal': create_splits(X_without_labs_minimal, y_regression.dropna(), train_idx, val_idx, test_idx),\n",
    "    'without_labs_full': create_splits(X_without_labs_full, y_regression.dropna(), train_idx, val_idx, test_idx),\n",
    "}\n",
    "\n",
    "print(\"Classification split sizes:\")\n",
    "for name, (X_tr, X_v, X_te, y_tr, y_v, y_te) in splits_cls.items():\n",
    "    print(f\"  {name}: train={len(X_tr)}, val={len(X_v)}, test={len(X_te)}\")\n",
    "\n",
    "print(\"\\nRegression split sizes (fewer samples due to missing HbA1c):\")\n",
    "for name, (X_tr, X_v, X_te, y_tr, y_v, y_te) in splits_reg.items():\n",
    "    print(f\"  {name}: train={len(X_tr)}, val={len(X_v)}, test={len(X_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for handling imbalance\n",
    "class_weights = get_class_weights(y_train_cls)\n",
    "print(\"Class weights (for balanced training):\")\n",
    "for cls, weight in class_weights.items():\n",
    "    print(f\"  {status_labels[cls]}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup MLflow\n",
    "\n",
    "MLflow is an open-source platform for managing the ML lifecycle. We use it to:\n",
    "\n",
    "1. **Track experiments**: Each model training run is logged with:\n",
    "   - Parameters (hyperparameters)\n",
    "   - Metrics (F1, AUC, RMSE, etc.)\n",
    "   - Artifacts (saved models, plots)\n",
    "   - Tags (model type, feature set)\n",
    "\n",
    "2. **Compare models**: The MLflow UI lets us compare runs side-by-side\n",
    "\n",
    "3. **Reproduce results**: Every run is logged with exact parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow tracking\n",
    "mlflow.set_tracking_uri(str(MLRUNS_DIR))\n",
    "experiment_name = 'diabetes-prediction-phase7'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow tracking URI: {MLRUNS_DIR}\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "print(f\"\\nTo view results, run:\")\n",
    "print(f\"  cd {project_root}\")\n",
    "print(f\"  mlflow ui --backend-store-uri mlruns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBM Training\n",
    "\n",
    "LightGBM is a gradient boosting framework that:\n",
    "- **Handles NaN natively**: Uses `minimal` imputation datasets\n",
    "- **Fast training**: Uses histogram-based algorithm\n",
    "- **Good for tabular data**: Often best performer for structured data\n",
    "\n",
    "### Hyperparameters tuned with Optuna:\n",
    "- `n_estimators`: Number of boosting rounds (100-500)\n",
    "- `max_depth`: Maximum tree depth (3-10)\n",
    "- `learning_rate`: Step size shrinkage (0.01-0.3)\n",
    "- `num_leaves`: Maximum leaves per tree (15-127)\n",
    "- `min_child_samples`: Minimum samples in leaf (5-100)\n",
    "- `reg_alpha`, `reg_lambda`: L1/L2 regularization\n",
    "- `subsample`, `colsample_bytree`: Row/column sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=100,\n",
    "    class_weights=None,\n",
    "    cv_folds=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train LightGBM with Optuna hyperparameter optimization.\n",
    "    \n",
    "    Uses 3-fold CV during tuning, then retrains best model on full train set.\n",
    "    \"\"\"\n",
    "    # Scoring metric\n",
    "    scoring = 'f1_macro' if task == 'classification' else 'neg_root_mean_squared_error'\n",
    "    \n",
    "    # CV splitter\n",
    "    if task == 'classification':\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Convert to numpy for sklearn compatibility\n",
    "    X_train_np = X_train.values\n",
    "    y_train_np = y_train.values\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 15, 127),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': -1,\n",
    "        }\n",
    "        \n",
    "        if task == 'classification':\n",
    "            params['class_weight'] = class_weights\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "        else:\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train_np, y_train_np, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "            return scores.mean()\n",
    "        except Exception as e:\n",
    "            return -1e10 if 'neg' in scoring else 0.0\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting Optuna optimization ({n_trials} trials)...\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = RANDOM_STATE\n",
    "    best_params['verbose'] = -1\n",
    "    best_params['n_jobs'] = -1\n",
    "    \n",
    "    print(f\"\\nBest trial score: {study.best_value:.4f}\")\n",
    "    print(f\"Best parameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Train final model on full training data\n",
    "    if task == 'classification':\n",
    "        best_params['class_weight'] = class_weights\n",
    "        best_model = lgb.LGBMClassifier(**best_params)\n",
    "    else:\n",
    "        best_model = lgb.LGBMRegressor(**best_params)\n",
    "    \n",
    "    best_model.fit(X_train_np, y_train_np)\n",
    "    \n",
    "    return best_model, best_params, study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LightGBM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for comparison\n",
    "results_classification = {}\n",
    "models_classification = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM Classification - With Labs\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Classification - WITH LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_minimal']\n",
    "\n",
    "lgb_clf_with_labs, lgb_clf_params_with_labs, study_lgb_clf_with = train_lightgbm_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=N_TRIALS_LIGHTGBM,\n",
    "    class_weights=class_weights,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = lgb_clf_with_labs.predict(X_val.values)\n",
    "y_prob = lgb_clf_with_labs.predict_proba(X_val.values)\n",
    "metrics = compute_classification_metrics(y_val.values, y_pred, y_prob)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {metrics['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='LightGBM_cls_with_labs'):\n",
    "    mlflow.set_tag('model_type', 'LightGBM')\n",
    "    mlflow.set_tag('task', 'classification')\n",
    "    mlflow.set_tag('feature_set', 'with_labs')\n",
    "    mlflow.set_tag('imputation', 'minimal')\n",
    "    \n",
    "    # Log parameters\n",
    "    for k, v in lgb_clf_params_with_labs.items():\n",
    "        if k not in ['class_weight', 'verbose', 'n_jobs']:\n",
    "            mlflow.log_param(k, v)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric('val_accuracy', metrics['accuracy'])\n",
    "    mlflow.log_metric('val_f1_macro', metrics['f1_macro'])\n",
    "    mlflow.log_metric('val_roc_auc_ovr', metrics['roc_auc_ovr'])\n",
    "    mlflow.log_metric('best_cv_score', study_lgb_clf_with.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_LIGHTGBM)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(lgb_clf_with_labs, 'model')\n",
    "\n",
    "# Store results\n",
    "results_classification['LightGBM (with labs)'] = metrics\n",
    "models_classification['LightGBM (with labs)'] = lgb_clf_with_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM Classification - Without Labs\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Classification - WITHOUT LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_minimal']\n",
    "\n",
    "lgb_clf_without_labs, lgb_clf_params_without_labs, study_lgb_clf_without = train_lightgbm_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=N_TRIALS_LIGHTGBM,\n",
    "    class_weights=class_weights,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = lgb_clf_without_labs.predict(X_val.values)\n",
    "y_prob = lgb_clf_without_labs.predict_proba(X_val.values)\n",
    "metrics = compute_classification_metrics(y_val.values, y_pred, y_prob)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {metrics['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='LightGBM_cls_without_labs'):\n",
    "    mlflow.set_tag('model_type', 'LightGBM')\n",
    "    mlflow.set_tag('task', 'classification')\n",
    "    mlflow.set_tag('feature_set', 'without_labs')\n",
    "    mlflow.set_tag('imputation', 'minimal')\n",
    "    \n",
    "    for k, v in lgb_clf_params_without_labs.items():\n",
    "        if k not in ['class_weight', 'verbose', 'n_jobs']:\n",
    "            mlflow.log_param(k, v)\n",
    "    \n",
    "    mlflow.log_metric('val_accuracy', metrics['accuracy'])\n",
    "    mlflow.log_metric('val_f1_macro', metrics['f1_macro'])\n",
    "    mlflow.log_metric('val_roc_auc_ovr', metrics['roc_auc_ovr'])\n",
    "    mlflow.log_metric('best_cv_score', study_lgb_clf_without.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_LIGHTGBM)\n",
    "    \n",
    "    mlflow.sklearn.log_model(lgb_clf_without_labs, 'model')\n",
    "\n",
    "results_classification['LightGBM (without labs)'] = metrics\n",
    "models_classification['LightGBM (without labs)'] = lgb_clf_without_labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LightGBM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store regression results\n",
    "results_regression = {}\n",
    "models_regression = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM Regression - With Labs\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Regression - WITH LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['with_labs_minimal']\n",
    "\n",
    "lgb_reg_with_labs, lgb_reg_params_with_labs, study_lgb_reg_with = train_lightgbm_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='regression',\n",
    "    n_trials=N_TRIALS_LIGHTGBM,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = lgb_reg_with_labs.predict(X_val.values)\n",
    "metrics = compute_regression_metrics(y_val.values, y_pred)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='LightGBM_reg_with_labs'):\n",
    "    mlflow.set_tag('model_type', 'LightGBM')\n",
    "    mlflow.set_tag('task', 'regression')\n",
    "    mlflow.set_tag('feature_set', 'with_labs')\n",
    "    mlflow.set_tag('imputation', 'minimal')\n",
    "    \n",
    "    for k, v in lgb_reg_params_with_labs.items():\n",
    "        if k not in ['verbose', 'n_jobs']:\n",
    "            mlflow.log_param(k, v)\n",
    "    \n",
    "    mlflow.log_metric('val_rmse', metrics['rmse'])\n",
    "    mlflow.log_metric('val_mae', metrics['mae'])\n",
    "    mlflow.log_metric('val_r2', metrics['r2'])\n",
    "    mlflow.log_metric('best_cv_score', study_lgb_reg_with.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_LIGHTGBM)\n",
    "    \n",
    "    mlflow.sklearn.log_model(lgb_reg_with_labs, 'model')\n",
    "\n",
    "results_regression['LightGBM (with labs)'] = metrics\n",
    "models_regression['LightGBM (with labs)'] = lgb_reg_with_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM Regression - Without Labs\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Regression - WITHOUT LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['without_labs_minimal']\n",
    "\n",
    "lgb_reg_without_labs, lgb_reg_params_without_labs, study_lgb_reg_without = train_lightgbm_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='regression',\n",
    "    n_trials=N_TRIALS_LIGHTGBM,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = lgb_reg_without_labs.predict(X_val.values)\n",
    "metrics = compute_regression_metrics(y_val.values, y_pred)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='LightGBM_reg_without_labs'):\n",
    "    mlflow.set_tag('model_type', 'LightGBM')\n",
    "    mlflow.set_tag('task', 'regression')\n",
    "    mlflow.set_tag('feature_set', 'without_labs')\n",
    "    mlflow.set_tag('imputation', 'minimal')\n",
    "    \n",
    "    for k, v in lgb_reg_params_without_labs.items():\n",
    "        if k not in ['verbose', 'n_jobs']:\n",
    "            mlflow.log_param(k, v)\n",
    "    \n",
    "    mlflow.log_metric('val_rmse', metrics['rmse'])\n",
    "    mlflow.log_metric('val_mae', metrics['mae'])\n",
    "    mlflow.log_metric('val_r2', metrics['r2'])\n",
    "    mlflow.log_metric('best_cv_score', study_lgb_reg_without.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_LIGHTGBM)\n",
    "    \n",
    "    mlflow.sklearn.log_model(lgb_reg_without_labs, 'model')\n",
    "\n",
    "results_regression['LightGBM (without labs)'] = metrics\n",
    "models_regression['LightGBM (without labs)'] = lgb_reg_without_labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLP Training\n",
    "\n",
    "Multi-Layer Perceptron (sklearn implementation):\n",
    "- **Requires complete data**: Uses `full` imputation datasets (no NaN)\n",
    "- **Requires scaling**: Features must be standardized\n",
    "- **Flexible architecture**: Number of layers and neurons tuned by Optuna\n",
    "\n",
    "### Hyperparameters tuned:\n",
    "- `n_layers`: Number of hidden layers (1-3)\n",
    "- `n_units_l{i}`: Neurons per layer (32-256)\n",
    "- `activation`: relu or tanh\n",
    "- `alpha`: L2 regularization strength\n",
    "- `learning_rate`: constant or adaptive\n",
    "- `learning_rate_init`: Initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=50,\n",
    "    cv_folds=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train MLP with Optuna hyperparameter optimization.\n",
    "    \n",
    "    Note: MLP requires standardized features, so we fit a scaler on training data.\n",
    "    \"\"\"\n",
    "    # Standardize features (critical for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Scoring metric\n",
    "    scoring = 'f1_macro' if task == 'classification' else 'neg_root_mean_squared_error'\n",
    "    \n",
    "    # CV splitter\n",
    "    if task == 'classification':\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Layer architecture\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(trial.suggest_int(f'n_units_l{i}', 32, 256))\n",
    "        \n",
    "        params = {\n",
    "            'hidden_layer_sizes': tuple(layers),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
    "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "            'max_iter': 500,\n",
    "            'early_stopping': True,\n",
    "            'validation_fraction': 0.1,\n",
    "            'n_iter_no_change': 20,\n",
    "            'random_state': RANDOM_STATE,\n",
    "        }\n",
    "        \n",
    "        if task == 'classification':\n",
    "            model = MLPClassifier(**params)\n",
    "        else:\n",
    "            model = MLPRegressor(**params)\n",
    "        \n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train_scaled, y_train_np, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "            return scores.mean()\n",
    "        except Exception as e:\n",
    "            return -1e10 if 'neg' in scoring else 0.0\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting Optuna optimization ({n_trials} trials)...\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Reconstruct layer sizes\n",
    "    n_layers = best_params['n_layers']\n",
    "    layers = [best_params[f'n_units_l{i}'] for i in range(n_layers)]\n",
    "    \n",
    "    model_params = {\n",
    "        'hidden_layer_sizes': tuple(layers),\n",
    "        'activation': best_params['activation'],\n",
    "        'alpha': best_params['alpha'],\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'learning_rate_init': best_params['learning_rate_init'],\n",
    "        'max_iter': 500,\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.1,\n",
    "        'n_iter_no_change': 20,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBest trial score: {study.best_value:.4f}\")\n",
    "    print(f\"Best parameters:\")\n",
    "    print(f\"  hidden_layer_sizes: {layers}\")\n",
    "    print(f\"  activation: {model_params['activation']}\")\n",
    "    print(f\"  alpha: {model_params['alpha']:.6f}\")\n",
    "    print(f\"  learning_rate: {model_params['learning_rate']}\")\n",
    "    print(f\"  learning_rate_init: {model_params['learning_rate_init']:.6f}\")\n",
    "    \n",
    "    # Train final model\n",
    "    if task == 'classification':\n",
    "        best_model = MLPClassifier(**model_params)\n",
    "    else:\n",
    "        best_model = MLPRegressor(**model_params)\n",
    "    \n",
    "    best_model.fit(X_train_scaled, y_train_np)\n",
    "    \n",
    "    return best_model, model_params, study, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MLP Classification - With Labs\n",
    "print(\"=\"*60)\n",
    "print(\"MLP Classification - WITH LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_full']\n",
    "\n",
    "mlp_clf_with_labs, mlp_clf_params_with_labs, study_mlp_clf_with, scaler_clf_with = train_mlp_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=N_TRIALS_MLP,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set (must scale!)\n",
    "X_val_scaled = scaler_clf_with.transform(X_val)\n",
    "y_pred = mlp_clf_with_labs.predict(X_val_scaled)\n",
    "y_prob = mlp_clf_with_labs.predict_proba(X_val_scaled)\n",
    "metrics = compute_classification_metrics(y_val.values, y_pred, y_prob)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {metrics['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='MLP_cls_with_labs'):\n",
    "    mlflow.set_tag('model_type', 'MLP')\n",
    "    mlflow.set_tag('task', 'classification')\n",
    "    mlflow.set_tag('feature_set', 'with_labs')\n",
    "    mlflow.set_tag('imputation', 'full')\n",
    "    \n",
    "    mlflow.log_param('hidden_layer_sizes', str(mlp_clf_params_with_labs['hidden_layer_sizes']))\n",
    "    mlflow.log_param('activation', mlp_clf_params_with_labs['activation'])\n",
    "    mlflow.log_param('alpha', mlp_clf_params_with_labs['alpha'])\n",
    "    mlflow.log_param('learning_rate', mlp_clf_params_with_labs['learning_rate'])\n",
    "    mlflow.log_param('learning_rate_init', mlp_clf_params_with_labs['learning_rate_init'])\n",
    "    \n",
    "    mlflow.log_metric('val_accuracy', metrics['accuracy'])\n",
    "    mlflow.log_metric('val_f1_macro', metrics['f1_macro'])\n",
    "    mlflow.log_metric('val_roc_auc_ovr', metrics['roc_auc_ovr'])\n",
    "    mlflow.log_metric('best_cv_score', study_mlp_clf_with.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_MLP)\n",
    "    \n",
    "    mlflow.sklearn.log_model(mlp_clf_with_labs, 'model')\n",
    "\n",
    "results_classification['MLP (with labs)'] = metrics\n",
    "models_classification['MLP (with labs)'] = (mlp_clf_with_labs, scaler_clf_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MLP Classification - Without Labs\n",
    "print(\"=\"*60)\n",
    "print(\"MLP Classification - WITHOUT LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_full']\n",
    "\n",
    "mlp_clf_without_labs, mlp_clf_params_without_labs, study_mlp_clf_without, scaler_clf_without = train_mlp_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='classification',\n",
    "    n_trials=N_TRIALS_MLP,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "X_val_scaled = scaler_clf_without.transform(X_val)\n",
    "y_pred = mlp_clf_without_labs.predict(X_val_scaled)\n",
    "y_prob = mlp_clf_without_labs.predict_proba(X_val_scaled)\n",
    "metrics = compute_classification_metrics(y_val.values, y_pred, y_prob)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {metrics['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='MLP_cls_without_labs'):\n",
    "    mlflow.set_tag('model_type', 'MLP')\n",
    "    mlflow.set_tag('task', 'classification')\n",
    "    mlflow.set_tag('feature_set', 'without_labs')\n",
    "    mlflow.set_tag('imputation', 'full')\n",
    "    \n",
    "    mlflow.log_param('hidden_layer_sizes', str(mlp_clf_params_without_labs['hidden_layer_sizes']))\n",
    "    mlflow.log_param('activation', mlp_clf_params_without_labs['activation'])\n",
    "    mlflow.log_param('alpha', mlp_clf_params_without_labs['alpha'])\n",
    "    mlflow.log_param('learning_rate', mlp_clf_params_without_labs['learning_rate'])\n",
    "    mlflow.log_param('learning_rate_init', mlp_clf_params_without_labs['learning_rate_init'])\n",
    "    \n",
    "    mlflow.log_metric('val_accuracy', metrics['accuracy'])\n",
    "    mlflow.log_metric('val_f1_macro', metrics['f1_macro'])\n",
    "    mlflow.log_metric('val_roc_auc_ovr', metrics['roc_auc_ovr'])\n",
    "    mlflow.log_metric('best_cv_score', study_mlp_clf_without.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_MLP)\n",
    "    \n",
    "    mlflow.sklearn.log_model(mlp_clf_without_labs, 'model')\n",
    "\n",
    "results_classification['MLP (without labs)'] = metrics\n",
    "models_classification['MLP (without labs)'] = (mlp_clf_without_labs, scaler_clf_without)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MLP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MLP Regression - With Labs\n",
    "print(\"=\"*60)\n",
    "print(\"MLP Regression - WITH LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['with_labs_full']\n",
    "\n",
    "mlp_reg_with_labs, mlp_reg_params_with_labs, study_mlp_reg_with, scaler_reg_with = train_mlp_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='regression',\n",
    "    n_trials=N_TRIALS_MLP,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "X_val_scaled = scaler_reg_with.transform(X_val)\n",
    "y_pred = mlp_reg_with_labs.predict(X_val_scaled)\n",
    "metrics = compute_regression_metrics(y_val.values, y_pred)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='MLP_reg_with_labs'):\n",
    "    mlflow.set_tag('model_type', 'MLP')\n",
    "    mlflow.set_tag('task', 'regression')\n",
    "    mlflow.set_tag('feature_set', 'with_labs')\n",
    "    mlflow.set_tag('imputation', 'full')\n",
    "    \n",
    "    mlflow.log_param('hidden_layer_sizes', str(mlp_reg_params_with_labs['hidden_layer_sizes']))\n",
    "    mlflow.log_param('activation', mlp_reg_params_with_labs['activation'])\n",
    "    mlflow.log_param('alpha', mlp_reg_params_with_labs['alpha'])\n",
    "    mlflow.log_param('learning_rate', mlp_reg_params_with_labs['learning_rate'])\n",
    "    mlflow.log_param('learning_rate_init', mlp_reg_params_with_labs['learning_rate_init'])\n",
    "    \n",
    "    mlflow.log_metric('val_rmse', metrics['rmse'])\n",
    "    mlflow.log_metric('val_mae', metrics['mae'])\n",
    "    mlflow.log_metric('val_r2', metrics['r2'])\n",
    "    mlflow.log_metric('best_cv_score', study_mlp_reg_with.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_MLP)\n",
    "    \n",
    "    mlflow.sklearn.log_model(mlp_reg_with_labs, 'model')\n",
    "\n",
    "results_regression['MLP (with labs)'] = metrics\n",
    "models_regression['MLP (with labs)'] = (mlp_reg_with_labs, scaler_reg_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MLP Regression - Without Labs\n",
    "print(\"=\"*60)\n",
    "print(\"MLP Regression - WITHOUT LABS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['without_labs_full']\n",
    "\n",
    "mlp_reg_without_labs, mlp_reg_params_without_labs, study_mlp_reg_without, scaler_reg_without = train_mlp_with_optuna(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    task='regression',\n",
    "    n_trials=N_TRIALS_MLP,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "X_val_scaled = scaler_reg_without.transform(X_val)\n",
    "y_pred = mlp_reg_without_labs.predict(X_val_scaled)\n",
    "metrics = compute_regression_metrics(y_val.values, y_pred)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name='MLP_reg_without_labs'):\n",
    "    mlflow.set_tag('model_type', 'MLP')\n",
    "    mlflow.set_tag('task', 'regression')\n",
    "    mlflow.set_tag('feature_set', 'without_labs')\n",
    "    mlflow.set_tag('imputation', 'full')\n",
    "    \n",
    "    mlflow.log_param('hidden_layer_sizes', str(mlp_reg_params_without_labs['hidden_layer_sizes']))\n",
    "    mlflow.log_param('activation', mlp_reg_params_without_labs['activation'])\n",
    "    mlflow.log_param('alpha', mlp_reg_params_without_labs['alpha'])\n",
    "    mlflow.log_param('learning_rate', mlp_reg_params_without_labs['learning_rate'])\n",
    "    mlflow.log_param('learning_rate_init', mlp_reg_params_without_labs['learning_rate_init'])\n",
    "    \n",
    "    mlflow.log_metric('val_rmse', metrics['rmse'])\n",
    "    mlflow.log_metric('val_mae', metrics['mae'])\n",
    "    mlflow.log_metric('val_r2', metrics['r2'])\n",
    "    mlflow.log_metric('best_cv_score', study_mlp_reg_without.best_value)\n",
    "    mlflow.log_metric('n_trials', N_TRIALS_MLP)\n",
    "    \n",
    "    mlflow.sklearn.log_model(mlp_reg_without_labs, 'model')\n",
    "\n",
    "results_regression['MLP (without labs)'] = metrics\n",
    "models_regression['MLP (without labs)'] = (mlp_reg_without_labs, scaler_reg_without)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Set Comparison\n",
    "\n",
    "Compare all models on the validation set before final test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification comparison\n",
    "print(\"=\"*60)\n",
    "print(\"Classification Model Comparison (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cls_comparison = compare_models_table(\n",
    "    results_classification,\n",
    "    metrics=['accuracy', 'f1_macro', 'roc_auc_ovr', 'precision_macro', 'recall_macro'],\n",
    "    sort_by='f1_macro',\n",
    "    ascending=False\n",
    ")\n",
    "print(cls_comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression comparison\n",
    "print(\"=\"*60)\n",
    "print(\"Regression Model Comparison (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reg_comparison = compare_models_table(\n",
    "    results_regression,\n",
    "    metrics=['rmse', 'mae', 'r2'],\n",
    "    sort_by='rmse',\n",
    "    ascending=True\n",
    ")\n",
    "print(reg_comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification comparison\n",
    "fig = plot_model_comparison(\n",
    "    results_classification,\n",
    "    metrics=['accuracy', 'f1_macro', 'roc_auc_ovr'],\n",
    "    title='Classification Model Comparison (Validation Set)'\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / 'phase7_classification_comparison_val.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With vs Without Labs comparison\n",
    "results_with = {k: v for k, v in results_classification.items() if 'with labs' in k}\n",
    "results_without = {k.replace('without', 'with'): v for k, v in results_classification.items() if 'without' in k}\n",
    "\n",
    "fig = plot_feature_set_comparison(\n",
    "    results_with,\n",
    "    results_without,\n",
    "    metric='f1_macro',\n",
    "    title='F1 Macro: With Labs vs Without Labs'\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / 'phase7_with_without_labs_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Test Evaluation\n",
    "\n",
    "Evaluate best models on the held-out test set (never seen during training or hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all classification models on test set\n",
    "print(\"=\"*60)\n",
    "print(\"Classification Test Set Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results_cls = {}\n",
    "\n",
    "# LightGBM with labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_minimal']\n",
    "y_pred = lgb_clf_with_labs.predict(X_test.values)\n",
    "y_prob = lgb_clf_with_labs.predict_proba(X_test.values)\n",
    "test_results_cls['LightGBM (with labs)'] = compute_classification_metrics(y_test.values, y_pred, y_prob)\n",
    "\n",
    "# LightGBM without labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_minimal']\n",
    "y_pred = lgb_clf_without_labs.predict(X_test.values)\n",
    "y_prob = lgb_clf_without_labs.predict_proba(X_test.values)\n",
    "test_results_cls['LightGBM (without labs)'] = compute_classification_metrics(y_test.values, y_pred, y_prob)\n",
    "\n",
    "# MLP with labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_full']\n",
    "X_test_scaled = scaler_clf_with.transform(X_test)\n",
    "y_pred = mlp_clf_with_labs.predict(X_test_scaled)\n",
    "y_prob = mlp_clf_with_labs.predict_proba(X_test_scaled)\n",
    "test_results_cls['MLP (with labs)'] = compute_classification_metrics(y_test.values, y_pred, y_prob)\n",
    "\n",
    "# MLP without labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_full']\n",
    "X_test_scaled = scaler_clf_without.transform(X_test)\n",
    "y_pred = mlp_clf_without_labs.predict(X_test_scaled)\n",
    "y_prob = mlp_clf_without_labs.predict_proba(X_test_scaled)\n",
    "test_results_cls['MLP (without labs)'] = compute_classification_metrics(y_test.values, y_pred, y_prob)\n",
    "\n",
    "# Display comparison\n",
    "test_cls_comparison = compare_models_table(\n",
    "    test_results_cls,\n",
    "    metrics=['accuracy', 'f1_macro', 'roc_auc_ovr'],\n",
    "    sort_by='f1_macro',\n",
    "    ascending=False\n",
    ")\n",
    "print(test_cls_comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all regression models on test set\n",
    "print(\"=\"*60)\n",
    "print(\"Regression Test Set Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results_reg = {}\n",
    "\n",
    "# LightGBM with labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['with_labs_minimal']\n",
    "y_pred = lgb_reg_with_labs.predict(X_test.values)\n",
    "test_results_reg['LightGBM (with labs)'] = compute_regression_metrics(y_test.values, y_pred)\n",
    "\n",
    "# LightGBM without labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['without_labs_minimal']\n",
    "y_pred = lgb_reg_without_labs.predict(X_test.values)\n",
    "test_results_reg['LightGBM (without labs)'] = compute_regression_metrics(y_test.values, y_pred)\n",
    "\n",
    "# MLP with labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['with_labs_full']\n",
    "X_test_scaled = scaler_reg_with.transform(X_test)\n",
    "y_pred = mlp_reg_with_labs.predict(X_test_scaled)\n",
    "test_results_reg['MLP (with labs)'] = compute_regression_metrics(y_test.values, y_pred)\n",
    "\n",
    "# MLP without labs\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits_reg['without_labs_full']\n",
    "X_test_scaled = scaler_reg_without.transform(X_test)\n",
    "y_pred = mlp_reg_without_labs.predict(X_test_scaled)\n",
    "test_results_reg['MLP (without labs)'] = compute_regression_metrics(y_test.values, y_pred)\n",
    "\n",
    "# Display comparison\n",
    "test_reg_comparison = compare_models_table(\n",
    "    test_results_reg,\n",
    "    metrics=['rmse', 'mae', 'r2'],\n",
    "    sort_by='rmse',\n",
    "    ascending=True\n",
    ")\n",
    "print(test_reg_comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set comparison\n",
    "fig = plot_model_comparison(\n",
    "    test_results_cls,\n",
    "    metrics=['accuracy', 'f1_macro', 'roc_auc_ovr'],\n",
    "    title='Classification Model Comparison (Test Set)'\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / 'phase7_classification_comparison_test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Model Analysis\n",
    "\n",
    "Generate detailed evaluation for the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best classification model\n",
    "best_cls_model_name = max(test_results_cls, key=lambda x: test_results_cls[x]['f1_macro'])\n",
    "print(f\"Best Classification Model: {best_cls_model_name}\")\n",
    "print(f\"  F1 Macro: {test_results_cls[best_cls_model_name]['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {test_results_cls[best_cls_model_name]['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "# Find best regression model\n",
    "best_reg_model_name = min(test_results_reg, key=lambda x: test_results_reg[x]['rmse'])\n",
    "print(f\"\\nBest Regression Model: {best_reg_model_name}\")\n",
    "print(f\"  RMSE: {test_results_reg[best_reg_model_name]['rmse']:.4f}\")\n",
    "print(f\"  R²:   {test_results_reg[best_reg_model_name]['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for best classification model\n",
    "if 'LightGBM' in best_cls_model_name and 'with labs' in best_cls_model_name:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_minimal']\n",
    "    y_pred = lgb_clf_with_labs.predict(X_test.values)\n",
    "    y_prob = lgb_clf_with_labs.predict_proba(X_test.values)\n",
    "elif 'LightGBM' in best_cls_model_name:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_minimal']\n",
    "    y_pred = lgb_clf_without_labs.predict(X_test.values)\n",
    "    y_prob = lgb_clf_without_labs.predict_proba(X_test.values)\n",
    "elif 'with labs' in best_cls_model_name:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['with_labs_full']\n",
    "    X_test_scaled = scaler_clf_with.transform(X_test)\n",
    "    y_pred = mlp_clf_with_labs.predict(X_test_scaled)\n",
    "    y_prob = mlp_clf_with_labs.predict_proba(X_test_scaled)\n",
    "else:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = splits_cls['without_labs_full']\n",
    "    X_test_scaled = scaler_clf_without.transform(X_test)\n",
    "    y_pred = mlp_clf_without_labs.predict(X_test_scaled)\n",
    "    y_prob = mlp_clf_without_labs.predict_proba(X_test_scaled)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_confusion_matrix(y_test.values, y_pred, ax=axes[0], \n",
    "                     title=f'{best_cls_model_name} - Confusion Matrix')\n",
    "plot_confusion_matrix(y_test.values, y_pred, ax=axes[1], normalize=True,\n",
    "                     title=f'{best_cls_model_name} - Normalized')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'phase7_best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for best model\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_roc_curves(y_test.values, y_prob, ax=ax, \n",
    "               title=f'{best_cls_model_name} - ROC Curves')\n",
    "fig.savefig(FIGURES_DIR / 'phase7_best_model_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for best model\n",
    "print(f\"\\n{best_cls_model_name} - Classification Report (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_test.values, y_pred,\n",
    "    target_names=['No Diabetes', 'Prediabetes', 'Diabetes']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "advanced_models_dir = MODELS_DIR / 'advanced'\n",
    "advanced_models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Classification models\n",
    "cls_dir = advanced_models_dir / 'classification'\n",
    "cls_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(lgb_clf_with_labs, cls_dir / 'lgb_with_labs.joblib')\n",
    "joblib.dump(lgb_clf_without_labs, cls_dir / 'lgb_without_labs.joblib')\n",
    "joblib.dump((mlp_clf_with_labs, scaler_clf_with), cls_dir / 'mlp_with_labs.joblib')\n",
    "joblib.dump((mlp_clf_without_labs, scaler_clf_without), cls_dir / 'mlp_without_labs.joblib')\n",
    "\n",
    "# Regression models\n",
    "reg_dir = advanced_models_dir / 'regression'\n",
    "reg_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(lgb_reg_with_labs, reg_dir / 'lgb_with_labs.joblib')\n",
    "joblib.dump(lgb_reg_without_labs, reg_dir / 'lgb_without_labs.joblib')\n",
    "joblib.dump((mlp_reg_with_labs, scaler_reg_with), reg_dir / 'mlp_with_labs.joblib')\n",
    "joblib.dump((mlp_reg_without_labs, scaler_reg_without), reg_dir / 'mlp_without_labs.joblib')\n",
    "\n",
    "print(f\"Models saved to {advanced_models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results summary\n",
    "results_summary = {\n",
    "    'classification': {\n",
    "        'validation': {k: {m: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                          for m, v in metrics.items() if m != 'confusion_matrix'}\n",
    "                      for k, metrics in results_classification.items()},\n",
    "        'test': {k: {m: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                    for m, v in metrics.items() if m != 'confusion_matrix'}\n",
    "                for k, metrics in test_results_cls.items()},\n",
    "    },\n",
    "    'regression': {\n",
    "        'validation': {k: {m: float(v) for m, v in metrics.items()}\n",
    "                      for k, metrics in results_regression.items()},\n",
    "        'test': {k: {m: float(v) for m, v in metrics.items()}\n",
    "                for k, metrics in test_results_reg.items()},\n",
    "    },\n",
    "    'best_models': {\n",
    "        'classification': best_cls_model_name,\n",
    "        'regression': best_reg_model_name,\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'lightgbm_cls_with_labs': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                                   for k, v in lgb_clf_params_with_labs.items() \n",
    "                                   if k not in ['class_weight', 'verbose', 'n_jobs', 'random_state']},\n",
    "        'lightgbm_cls_without_labs': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                                      for k, v in lgb_clf_params_without_labs.items() \n",
    "                                      if k not in ['class_weight', 'verbose', 'n_jobs', 'random_state']},\n",
    "        'mlp_cls_with_labs': {k: str(v) if k == 'hidden_layer_sizes' else v \n",
    "                              for k, v in mlp_clf_params_with_labs.items() \n",
    "                              if k not in ['max_iter', 'early_stopping', 'validation_fraction', 'n_iter_no_change', 'random_state']},\n",
    "    },\n",
    "    'config': {\n",
    "        'n_trials_lightgbm': N_TRIALS_LIGHTGBM,\n",
    "        'n_trials_mlp': N_TRIALS_MLP,\n",
    "        'cv_folds': CV_FOLDS,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(advanced_models_dir / 'results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {advanced_models_dir / 'results_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Phase 7: Advanced Models - Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n### Classification Results (Test Set)\")\n",
    "print(test_cls_comparison.to_string())\n",
    "\n",
    "print(\"\\n### Regression Results (Test Set)\")\n",
    "print(test_reg_comparison.to_string())\n",
    "\n",
    "print(f\"\\n### Best Models\")\n",
    "print(f\"Classification: {best_cls_model_name}\")\n",
    "print(f\"  F1 Macro: {test_results_cls[best_cls_model_name]['f1_macro']:.4f}\")\n",
    "print(f\"  ROC AUC:  {test_results_cls[best_cls_model_name]['roc_auc_ovr']:.4f}\")\n",
    "print(f\"\\nRegression: {best_reg_model_name}\")\n",
    "print(f\"  RMSE: {test_results_reg[best_reg_model_name]['rmse']:.4f}\")\n",
    "print(f\"  R²:   {test_results_reg[best_reg_model_name]['r2']:.4f}\")\n",
    "\n",
    "print(\"\\n### Artifacts Generated\")\n",
    "print(f\"- Models saved to: {advanced_models_dir}\")\n",
    "print(f\"- Figures saved to: {FIGURES_DIR}\")\n",
    "print(f\"- MLflow runs logged to: {MLRUNS_DIR}\")\n",
    "print(f\"\\nTo view MLflow UI: mlflow ui --backend-store-uri {MLRUNS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Phase 7.1 (Optional)**: Deep Learning with TensorFlow/PyTorch\n",
    "- **Phase 8**: Model Evaluation & Comparison (detailed analysis)\n",
    "- **Phase 9**: Model Interpretation & Insights (SHAP, feature importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}